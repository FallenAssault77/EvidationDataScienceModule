{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook conducts what's known as an *ETL* pipeline. ETL stand for *Extract-Transform-Load*. Data science projects almost always begin by deciding what data we need to conduct an analysis, what transformations need to be performed on the data, and how the data should be stored for efficient manipulation/joining.\n",
    "\n",
    "In this notebook we will:\n",
    "- Pull down several raw datasets contained on the [NHANES website](https://en.wikipedia.org/wiki/Hierarchical_Data_Format). We'll do this using a combination of bash and pandas: bash for the datasets that need to be unzipped, pandas for those we can read directly from source.\n",
    "- Store the raw SAS format data as something more reasonable and modern (I'm going to use [HDF format](https://github.com/wesm/feather) which is a super fast binary data representation, but feel free to play around with .h5 or another format as you see fit)\n",
    "- Do some initial QC checks on the data to ensure everything looks reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Objectives\n",
    "\n",
    "After completing this module you should be able to do the following:\n",
    "\n",
    "1. Use pandas to pull raw data from source\n",
    "2. Use ```wget``` and ```unzip``` to pull in data that can't be read directly from source via pandas\n",
    "3. Understand why data compression is important, and how to downcast datatypes for efficient memory allocation\n",
    "4. Construct basic visualization tools for checking data quality\n",
    "5. Understand what metadata is, and build some tools for efficiently binding data with metadata\n",
    "6. Understand how to join together two relational tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize\n",
    "\n",
    "Below are the external dependencies this notebook depends on to function. If you don't have these installed you can install them using pip or conda- whichever you typically use as your package manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T17:17:50.895466Z",
     "start_time": "2018-03-28T17:17:50.327235Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import utils # Utility functions we will use\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "pd.set_option('max.rows', 100)\n",
    "pd.set_option('max.columns', 100)\n",
    "\n",
    "data_dir = '../data' # Change this location to the path where you would like your data saved to\n",
    "hdf_path = 'nhanes.h5' # Path to hdf store we will create later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download, unzip, and save raw physical activity monitor data.\n",
    "\n",
    "The raw PAX data is zipped due to its size and can't be read directly into pandas. We use the following steps to get it into a more manageable format:\n",
    "1. Download the data using the bash command ```wget```\n",
    "2. Unzip the downlaoded data using the bash command ```unzip```\n",
    "3. Read the data into pandas, and downcast data types to save on memory. The machine we are using has 32G of RAM, if your machine has less you may have issues with this step. If that's the case we suggest you don't actually run this portion of the ETL pipeline but rather use the ```nhanes.h5``` store we have written out and supplied to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-27T22:11:52.374Z"
    }
   },
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "# RUN THIS BLOCK ONLY IF YOU WANT TO RECREATE THE ETL PROCESS, OTHERWISE READ THE PREPROCESSED FILE #\n",
    "#####################################################################################################\n",
    "\n",
    "! wget https://wwwn.cdc.gov/Nchs/Nhanes/2005-2006/PAXRAW_D.ZIP\n",
    "! mv PAXRAW_D.ZIP {data_dir}\n",
    "! unzip {data_dir}PAXRAW_D.ZIP -d {data_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T23:06:09.692566Z",
     "start_time": "2018-03-27T23:06:09.657232Z"
    }
   },
   "outputs": [],
   "source": [
    "# Here we are reading the SAS file we pulled down to local. By default SAS stores all columns as float, \n",
    "# we want to downcast the data types into something less memory hungry.\n",
    "\n",
    "pax_raw = pd.read_sas(os.path.join(data_dir, 'paxraw_d.xpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T16:54:42.453813Z",
     "start_time": "2018-03-28T16:54:42.421436Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using the default float types is extremely memory intensive given the number of rows we are dealing with. \n",
    "# Let's downcast each column appropriately.\n",
    "\n",
    "type_map = {\n",
    "    'SEQN': np.int64,\n",
    "    'PAXSTAT': np.int8,\n",
    "    'PAXCAL': np.int8,\n",
    "    'PAXDAY': np.int8,\n",
    "    'PAXHOUR': np.int16,\n",
    "    'PAXMINUT': np.int16,\n",
    "    'PAXINTEN': np.int16,\n",
    "    'PAXSTEP': np.float32,\n",
    "    'PAXN': np.int16\n",
    "    \n",
    "}\n",
    "\n",
    "# Note this function mutates in place and does not return a copy\n",
    "def downcast_col_types(df, type_map):\n",
    "    for c in df.columns:\n",
    "        df[c] = df[c].astype(type_map[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T22:59:11.526917Z",
     "start_time": "2018-03-27T22:59:11.497365Z"
    }
   },
   "outputs": [],
   "source": [
    "downcast_col_types(pax_raw, type_map=type_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T16:56:08.247067Z",
     "start_time": "2018-03-28T16:54:47.582131Z"
    }
   },
   "outputs": [],
   "source": [
    "# Alternative to read_sas, read in the csv, with type_map passed in\n",
    "\n",
    "pax_raw = pd.read_csv(os.path.join(data_dir, 'paxraw_d.csv'), dtype=type_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T16:56:28.086243Z",
     "start_time": "2018-03-28T16:56:28.014575Z"
    }
   },
   "outputs": [],
   "source": [
    "# pythonize column names\n",
    "pax_raw.columns = [x.lower() for x in pax_raw.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T16:56:29.940998Z",
     "start_time": "2018-03-28T16:56:29.897289Z"
    }
   },
   "outputs": [],
   "source": [
    "# Display dataframe types and usage statistics\n",
    "pax_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that downcasting our types saved us a *significant* amount of memory. We manually inferred the best datatypes, but for datasets with larger numbers of columns it would be more efficient to automatically infer the datatypes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will store our data in an HDF store. An HDF store is very similar to a database like SQLite or PostgreSQL in that you can store several tables under one store. Additionally, HDF preserves python data types between write and read- which is a very nice property for dates or in our case memory savings. You can see in the code block below that we can list the table keys stored in our HDF store. Finally, I/O operations are *much* faster than file formats such as csv, sometimes by as much as 100x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T16:57:09.560643Z",
     "start_time": "2018-03-28T16:57:00.408018Z"
    }
   },
   "outputs": [],
   "source": [
    "pax_raw.to_hdf(os.path.join(data_dir, hdf_path), 'pax_raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read other NHANES tables directly from source\n",
    "\n",
    "The other tables aren't compressed- they are simply raw .XPT files written out via SAS. We can read directly from source. There are a TON of interesting files on the NHANES website- from disease state questionnaires to physical activity questionnaires. We have linked a few interesting tables below, we will leave it to you to add the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T16:57:16.572558Z",
     "start_time": "2018-03-28T16:57:16.543007Z"
    }
   },
   "outputs": [],
   "source": [
    "# In order to grab tables from the CDC NHANES website, simply add a table alias to the dictionary below,\n",
    "# and then reference the table link address as the corresponding key value. The read_sas_write_feather\n",
    "# function in utils will then read each sas file and write it to write_dir in feather format using the alias as the file name\n",
    "\n",
    "source_paths = {\n",
    "    'medical_conditions_questionnaire': 'https://wwwn.cdc.gov/Nchs/Nhanes/2005-2006/MCQ_D.XPT',\n",
    "    'weight_history_questionnaire': 'https://wwwn.cdc.gov/Nchs/Nhanes/2005-2006/WHQ_D.XPT',\n",
    "    'demographics_with_sample_weights': 'https://wwwn.cdc.gov/Nchs/Nhanes/2005-2006/DEMO_D.XPT'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to optimize I/O, and keep all of our data in one place, we'll download each of the raw datasets we are interested in into memory, then save it to our already created HDF store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T16:57:35.932194Z",
     "start_time": "2018-03-28T16:57:18.309613Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.read_sas_write_hdf(source_paths, data_dir, 'nhanes.h5', downcast=False, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T16:57:36.001442Z",
     "start_time": "2018-03-28T16:57:35.933937Z"
    }
   },
   "outputs": [],
   "source": [
    "with pd.HDFStore(os.path.join(data_dir, 'nhanes.h5')) as hdf:\n",
    "    print(hdf.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like all requested tables made it safely into our data store!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Find and read the physical activity questionnaire\n",
    "\n",
    "Find the raw SAS file on the 2005-2006 NHANES website that corresponds to the physical activity questionnaire. Use the function ```utils.read_sas_write_hdf()``` to download this file and write it into the HDF store that contains the other tables we already downloaded above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Find and read the physical activity questionnaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T16:58:02.126069Z",
     "start_time": "2018-03-28T16:58:02.099671Z"
    }
   },
   "outputs": [],
   "source": [
    "append_paths = {\n",
    "    'physical_activity_questionnaire': 'https://wwwn.cdc.gov/Nchs/Nhanes/2005-2006/PAQ_D.XPT'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T16:58:25.298224Z",
     "start_time": "2018-03-28T16:58:03.948146Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.read_sas_write_hdf(source_paths, data_dir, 'nhanes.h5', downcast=False, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial QC of physical activity data\n",
    "\n",
    "In order to verify that the raw physical activity data looks reasonable, we will create a some tools that will allow us to quickly visualize the time series that correspond to each user-day's walking patterns. There are a few good reasons for doing this:\n",
    "1. This will allow us to infer if our preprocessing worked correctly\n",
    "2. It will allow us to identify idiosyncracies in the raw data itself\n",
    "3. It will help us better understand what sorts of features could be useful for various prediction or infeence problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T17:18:16.971800Z",
     "start_time": "2018-03-28T17:18:12.276047Z"
    }
   },
   "outputs": [],
   "source": [
    "pax_raw = pd.read_hdf(os.path.join(data_dir, hdf_path), 'pax_raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T17:18:17.397769Z",
     "start_time": "2018-03-28T17:18:16.973724Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add a \"minute of day\" column as a plotting index\n",
    "pax_raw['minute_of_day'] = pax_raw.paxhour*60 + pax_raw.paxminut\n",
    "\n",
    "pax_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T23:59:44.341280Z",
     "start_time": "2018-03-27T23:59:42.301047Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take the raw data and filter out a single person just to check if the data looks reasonable.\n",
    "pax_sample = pax_raw[pax_raw.seqn==31128].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T00:00:03.960961Z",
     "start_time": "2018-03-28T00:00:03.252117Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the 30-minute rolling walk step series for our sampled user over day 1\n",
    "pax_sample[pax_sample.paxday==1].set_index('minute_of_day').paxstep.rolling(window=30, win_type='triang').mean().plot(figsize=(25,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just did some hand coding to plot a sngle user-day, but what if we wanted to explore at *many* user days? It would be helpful to build a tool that would allow us to quickly look at different days and different users in our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Plotting user day series\n",
    "\n",
    "Using the example code above, define a function ```plot_user_steps()``` that takes 4 arguments as input and plots a specific user's rolling walk step plot simialr to the plot shown above. The function's arguments should be:\n",
    "1. ```pax_df``` The physical activity dataframe (pandas dataframe)\n",
    "2. ```seqn_id``` The unique study participant id (int)\n",
    "3. ```day_of_study``` (int) The day of the study to plot\n",
    "4. ```window_len``` (int) The window length to use for the moving average.\n",
    "\n",
    "The function should then plot the rolling walk series for a specific user, for a specific day. As a bonus, build in functionality so that passing ```None``` to the parameter ```seqn_id``` will default to choosing a random id from the set of seqn ids and using that to create the plot.\n",
    "\n",
    "**Side Note: Building Data Science Tools**\n",
    "A lot of what practicing data scientists do on a daily basis revolves around building tools that solve specific problems pertaining to an analysis. In fact, some may argue that's what seperates a data scientist from your everyday analyst. During this module we really want to encourage you to build tools that make analysis scale. In the process you may even find that it can be very fun and rewarding to create a useful suite of analysis tools to speed up your workflow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Plotting user day series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrote a little snippet of code that implements the solution above. If you are curious how we did it, take a look in the ```utils``` folder that was imported at the top of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T17:05:15.410168Z",
     "start_time": "2018-03-28T17:05:08.141133Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.plot_user_steps(pax_raw, None, 2, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining Data\n",
    "\n",
    "The NHANES dataset has **A LOT** of tables. These tables are *relational* in structure meaning that their is a heirarchical structure that relates one set of data to another. In the NHANES dataset the *person level identifier* is the ```seqn``` number as you have already seen above. For some tables each row corresponds to exactly one ```seqn``` number. For other tables (like the raw activity data we have already worked with) there is a *one to many* mapping between ```seqn``` identifiers and row numbers, meaning that one ```seqn``` number corresponds to multiple input rows. Indeed for the ```pax_raw``` dataset we already worked with above, each person is associated with *thousands* of rows corresponding to each minute of a day in which the person wore an activity monitor.\n",
    "\n",
    "For this next section we will ensure that we can join together data from the various tables we pulled. This will be important for creating new interesting features later on, both for prediction and inference tasks, as we'll need to put together a feature matrix that combines data from multiple tables within the NHANES database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Joining Datasets\n",
    "\n",
    "As an example let's join together two of the tables we pulled in above that should now exist in your local HDF store:\n",
    "\n",
    "- Participant demographics\n",
    "- Physical activity data (we already pulled this in above)\n",
    "\n",
    "There is a 1:1 correspondence between these two tables. For this join, we will do what is known as an *inner join*. This means that we will specify a join key that exists in both sets, and *only* join those keys that exist in the intersection of the two key sets. For more information on the different types of joins check out [this resource](https://www.w3schools.com/sql/sql_join.asp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T17:18:22.325219Z",
     "start_time": "2018-03-28T17:18:22.290066Z"
    }
   },
   "outputs": [],
   "source": [
    "demo_df = pd.read_hdf(os.path.join(data_dir, hdf_path), 'demographics_with_sample_weights')\n",
    "\n",
    "# Let's checkout what's in this dataframe\n",
    "demo_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What in the world? What are all the cryptic column names?? Well, the CDC has chosen somewhat unhelpful names that map to demographic characteristics of participants. We *could* go to the website that lists the variable names and keep track of them. However, if we are clever there might be a better way. . . .The pandas library makes it esy to scrape tables via a url. All we need is the url where the table is located, as well as the index of the xml blob we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T17:18:23.977559Z",
     "start_time": "2018-03-28T17:18:23.953915Z"
    }
   },
   "outputs": [],
   "source": [
    "demo_metadata_url = 'https://wwwn.cdc.gov/nchs/nhanes/search/variablelist.aspx?Component=Demographics&CycleBeginYear=2005'\n",
    "idx = 1 # I looked up the blob I was interested in in advance\n",
    "\n",
    "demo_metadata = pd.read_html(demo_metadata_url)[idx]\n",
    "\n",
    "demo_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T18:05:05.216911Z",
     "start_time": "2018-03-28T18:05:05.173516Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CDEMO_AD', 'DEMO_D', 'N0506_GE', 'LDEMO_AD'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_metadata['Data File Name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Now we have a dataframe with two columns of interest: ```Variable Name``` and ```Variable Description```. We could remap the column names to human readable. I'll leave that as an exercise for you if you feel inclined. \n",
    "\n",
    "**Note: this metadata DataFrame has metadata from more files than just the `DEMO_D` data we are currently working with, so it might make sense to filter the rest out.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Create questionnaire metadata table\n",
    "\n",
    "For this exercise replicate the process we did above for the questionnaire metadata. You will need:\n",
    "- The url on the 2005 NHANES website where the questionnaire metadata is stored\n",
    "- The index of the xml blob corresponding to the metadata table\n",
    "\n",
    "Save the metadata table in a new dataframe called ```quest_metadata```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Create questionnaire metadata table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T17:24:08.739660Z",
     "start_time": "2018-03-28T17:24:08.715080Z"
    }
   },
   "outputs": [],
   "source": [
    "quest_metadata_url = 'https://wwwn.cdc.gov/nchs/nhanes/search/variablelist.aspx?Component=Questionnaire&CycleBeginYear=2005'\n",
    "quest_meta_data = pd.read_html(quest_metadata_url)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T21:28:21.446649Z",
     "start_time": "2018-03-28T21:28:21.383287Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable Name</th>\n",
       "      <th>Variable Description</th>\n",
       "      <th>Data File Name</th>\n",
       "      <th>Data File Description</th>\n",
       "      <th>Begin Year</th>\n",
       "      <th>EndYear</th>\n",
       "      <th>Component</th>\n",
       "      <th>Use Constraints</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SEQN</td>\n",
       "      <td>Respondent sequence number.</td>\n",
       "      <td>ACQ_D</td>\n",
       "      <td>Acculturation</td>\n",
       "      <td>2005</td>\n",
       "      <td>2006</td>\n",
       "      <td>Questionnaire</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACD010A</td>\n",
       "      <td>What language(s) {do you/does SP} usually spea...</td>\n",
       "      <td>ACQ_D</td>\n",
       "      <td>Acculturation</td>\n",
       "      <td>2005</td>\n",
       "      <td>2006</td>\n",
       "      <td>Questionnaire</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACD010B</td>\n",
       "      <td>What language(s) {do you/does SP} usually spea...</td>\n",
       "      <td>ACQ_D</td>\n",
       "      <td>Acculturation</td>\n",
       "      <td>2005</td>\n",
       "      <td>2006</td>\n",
       "      <td>Questionnaire</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACD010C</td>\n",
       "      <td>What language(s) {do you/does SP} usually spea...</td>\n",
       "      <td>ACQ_D</td>\n",
       "      <td>Acculturation</td>\n",
       "      <td>2005</td>\n",
       "      <td>2006</td>\n",
       "      <td>Questionnaire</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACD040</td>\n",
       "      <td>Now I'm going to ask you about language use. W...</td>\n",
       "      <td>ACQ_D</td>\n",
       "      <td>Acculturation</td>\n",
       "      <td>2005</td>\n",
       "      <td>2006</td>\n",
       "      <td>Questionnaire</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Variable Name                               Variable Description  \\\n",
       "0          SEQN                        Respondent sequence number.   \n",
       "1       ACD010A  What language(s) {do you/does SP} usually spea...   \n",
       "2       ACD010B  What language(s) {do you/does SP} usually spea...   \n",
       "3       ACD010C  What language(s) {do you/does SP} usually spea...   \n",
       "4        ACD040  Now I'm going to ask you about language use. W...   \n",
       "\n",
       "  Data File Name Data File Description  Begin Year  EndYear      Component  \\\n",
       "0          ACQ_D         Acculturation        2005     2006  Questionnaire   \n",
       "1          ACQ_D         Acculturation        2005     2006  Questionnaire   \n",
       "2          ACQ_D         Acculturation        2005     2006  Questionnaire   \n",
       "3          ACQ_D         Acculturation        2005     2006  Questionnaire   \n",
       "4          ACQ_D         Acculturation        2005     2006  Questionnaire   \n",
       "\n",
       "  Use Constraints  \n",
       "0            None  \n",
       "1            None  \n",
       "2            None  \n",
       "3            None  \n",
       "4            None  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quest_meta_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

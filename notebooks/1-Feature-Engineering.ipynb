{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "*Author: Benjamin Bradshaw*\n",
    "\n",
    "A *huge* piece of whether or not an inferential or predictive project is successful is whether or not the correct information is included in the model. Often times when dealing with IoT data, the raw signal collected from a device is not appropriate for direct input into the model. For example, if we were interested in using step data on a given day to predict whether or not that day was recorded on a weekday or a weekend, including each minute's step count for that day would result in 1440 features (machine learnign verbiage for variables). Unless we had a large amount of data to learn from and fed the raw signal into a complex model such as a neural network (which is possible these days) that might not be th ebest approach. A more labor intensive approach in a data constrained environment might be to conduct what is known as *feature engineering*.\n",
    "\n",
    "*Feature engineering* consists of taking raw data inputs and transforming them into such a way that extracts more signal, or captures specific characteristics of the data that are important for the task at hand. Feature engineering is not easy- mostly because it requires creativity on the part of *you* the practitioner to deeply understand the problem you are trying to solve, and then engineer features that allow a machine learning model to capture those characteristics. A large part of the preliminary detective work required for successful feature engineering comes in the form of *exploratory data analysis* which is just a fancy word for looking at the data and determining what aspects are important to capture in order to successfully extract the signal pertaining to your project.\n",
    "\n",
    "This notebook will cover the following topics:\n",
    "- Using exploratory data analysis (EDA) to identify useful aspects of the data that assist in our analysis task\n",
    "- Engineer features that capture the patterns identified during EDA\n",
    "\n",
    "In order motivate these tasks we construct the following prediction task: Can we utilize the accelerometer data in conjuction with other data available through NHANES to differentiate data collected on a weekday compare to data collected on a weekend?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning objectives\n",
    "\n",
    "By the time you work through this notebook you should be able to:\n",
    "- Understand the need for splitting our data into a training and testing set when implementing a classification task, as well as how to correctly split our data depending on the use case at hand\n",
    "- Use some of the basic libraries used in pythn for EDA including pandas, matplotlib, and seaborn\n",
    "- Build basic software that allows efficient scaling of data analysis\n",
    "- Understand how features may be extracted from raw accelerometer data for the purpose of a classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize\n",
    "\n",
    "Dependencies. . . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import utils # Utility functions containained in ./utils/\n",
    "import scipy\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "pd.set_option('max.rows', 100)\n",
    "pd.set_option('max.columns', 100)\n",
    "\n",
    "# Change this location to the path where you would like your data saved to\n",
    "data_dir = '/Users/bbradshaw/nhanes/'\n",
    "\n",
    "# Path to hdf store we will create later\n",
    "hdf_path = 'nhanes.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 956 ms, sys: 958 ms, total: 1.91 s\n",
      "Wall time: 2.26 s\n"
     ]
    }
   ],
   "source": [
    "# Load cleaned accelerometer data from last time\n",
    "%time pax = pd.read_hdf(os.path.join(data_dir, hdf_path), 'pax_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove 'pax' prefix on column names: it was getting annoying\n",
    "pax.columns = [x.split('pax')[1] if 'pax' in x else x for x in pax.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seqn</th>\n",
       "      <th>stat</th>\n",
       "      <th>cal</th>\n",
       "      <th>day</th>\n",
       "      <th>n</th>\n",
       "      <th>hour</th>\n",
       "      <th>minut</th>\n",
       "      <th>inten</th>\n",
       "      <th>step</th>\n",
       "      <th>minute_of_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31128.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>166</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31128.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31128.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31128.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>276</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31128.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      seqn  stat  cal  day  n  hour  minut  inten  step  minute_of_day\n",
       "0  31128.0     1    1    1  1     0      0    166   4.0              0\n",
       "1  31128.0     1    1    1  2     0      1     27   0.0              1\n",
       "2  31128.0     1    1    1  3     0      2      0   0.0              2\n",
       "3  31128.0     1    1    1  4     0      3    276   4.0              3\n",
       "4  31128.0     1    1    1  5     0      4      0   0.0              4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pax.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The need for a training and testing set\n",
    "\n",
    "Traditional statistics often times is concerned with creating *estimators* that are unbiased or perhaps consistent estimates of some true population parameter $\\theta$. In this setting it is common to \"fit\" an estimator (e.g. linear/logistic regression etc) to the *entire* sample of data. In this setting this choice makes sense, since we seek to minimize the difference between $\\theta$ and $\\hat{\\theta}$ (our estimated parameter). Certain estimators have guarantees about unbiasedness, and we maximize our statistical power when we use the entire sample to fit the model (think about what happens to the standard errors of the OLS coefficients as $n \\rightarrow \\infty$). However, at the end of the day an inferential analysis always suffers from doubt: apart from collecting a second (or third, or fourth) sample, we have no bullet proof way to check how robust our results are.\n",
    "\n",
    "In a *prediction* setting our goal is quite different. Given a set of inputs $X$ we want our model to do a \"good\" job of minimizing the \"difference\" between the actual label associated with that input $y$ and the predicted label the model produces $\\hat{y}$. Because this is our goal, we have a very powerful tool on our side to asses whether or not our model is \"good\". The tool is conceptually very simple and easy to understand: we will split our training set into two sets: a \"training\" set and a \"testing\" set. The training set will be used for all exploratory work, model training, model selection, hyperparameter tuning etc. The testing set will be set aside, completely quarantined from our development process, and only used once our model is entirely finished. At that point, we will use our model to compute predictions over the test set and we will use these predictions to assess just how good our model would have been \"out in the wild\" when making predictions on unseen examples.\n",
    "\n",
    "Constructing a test set is conceptually simple, but it is fraught with danger in implementation. How you construct this split requires careful forethought about the goal of the task, as well as attention to the details that might imperil the generalizeability of your model.\n",
    "\n",
    "In this section we will construct a train-test split of our dataset, walking through some of the common issues that you might want to consider when building a pipeline of your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a testing-training split\n",
    "\n",
    "Let's get into the details of creating a train-test split. First we need to decide what the classification use case is. We know we are trying to classify weekdays and weekend days, but do we want our model to generalize to *new unseen people* or do we want our model to generalize to *new unseen days* on the same people. There is a subtle distinction between those two cases. In the first case, we would want to make sure people observed in our training set are not observed in our test set in order to ensure we simulate the ability to generalize to new people. In the second case it's actually OK if the same people show up in both the training and testing splits: we are training the model to learn individual specific patterns and behaviors.\n",
    "\n",
    "For this analysis let's say our goal is option 1: generalize to new people. Since this is the case we need to ensure that days from people in our chosen training set do not also end up in our test set. We'll also want to ensure that the statistical properties of our training set match that of our test set as closely as possible. \n",
    "\n",
    "We'll start by taking a look at the demographic table found in NHANES. We'll then conduct the following steps:\n",
    "- Filter the demographics table to only include participants who also exist in the pax table\n",
    "- Conduct stratified sampling on age/gender/other important characteristics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining Data\n",
    "\n",
    "In order to do some interesting EDA, we probably want to bring in more data types than just the raw accelerometer data.\n",
    "\n",
    "The NHANES dataset has **A LOT** of tables. These tables are *relational* in structure meaning that their is a heirarchical structure that relates one set of data to another. In the NHANES dataset the *person level identifier* is the ```seqn``` number as you have already seen above. For some tables each row corresponds to exactly one ```seqn``` number. For other tables (like the raw activity data we have already worked with) there is a *one to many* mapping between ```seqn``` identifiers and row numbers, meaning that one ```seqn``` number corresponds to multiple input rows. Indeed for the ```pax``` dataset we already worked with above, each person is associated with *thousands* of rows corresponding to each minute of a day in which the person wore an activity monitor.\n",
    "\n",
    "For this next section we will ensure that we can join together data from the various tables we pulled. This will be important for creating new interesting features later on, both for prediction and inference tasks, as we'll need to put together a feature matrix that combines data from multiple tables within the NHANES database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Joining Datasets\n",
    "\n",
    "As an example let's join together two of the tables we pulled in above that should now exist in your local HDF store:\n",
    "\n",
    "- Participant demographics\n",
    "- Physical activity data (we already pulled this in above)\n",
    "\n",
    "There is a 1:1 correspondence between these two tables. For this join, we will do what is known as an *inner join*. This means that we will specify a join key that exists in both sets, and *only* join those keys that exist in the intersection of the two key sets. For more information on the different types of joins check out [this resource](https://www.w3schools.com/sql/sql_join.asp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seqn</th>\n",
       "      <th>sddsrvyr</th>\n",
       "      <th>ridstatr</th>\n",
       "      <th>ridexmon</th>\n",
       "      <th>riagendr</th>\n",
       "      <th>ridageyr</th>\n",
       "      <th>ridagemn</th>\n",
       "      <th>ridageex</th>\n",
       "      <th>ridreth1</th>\n",
       "      <th>dmqmilit</th>\n",
       "      <th>dmdborn</th>\n",
       "      <th>dmdcitzn</th>\n",
       "      <th>dmdyrsus</th>\n",
       "      <th>dmdeduc3</th>\n",
       "      <th>dmdeduc2</th>\n",
       "      <th>dmdschol</th>\n",
       "      <th>dmdmartl</th>\n",
       "      <th>dmdhhsiz</th>\n",
       "      <th>dmdfmsiz</th>\n",
       "      <th>indhhinc</th>\n",
       "      <th>indfminc</th>\n",
       "      <th>indfmpir</th>\n",
       "      <th>ridexprg</th>\n",
       "      <th>dmdhrgnd</th>\n",
       "      <th>dmdhrage</th>\n",
       "      <th>dmdhrbrn</th>\n",
       "      <th>dmdhredu</th>\n",
       "      <th>dmdhrmar</th>\n",
       "      <th>dmdhsedu</th>\n",
       "      <th>sialang</th>\n",
       "      <th>siaproxy</th>\n",
       "      <th>siaintrp</th>\n",
       "      <th>fialang</th>\n",
       "      <th>fiaproxy</th>\n",
       "      <th>fiaintrp</th>\n",
       "      <th>mialang</th>\n",
       "      <th>miaproxy</th>\n",
       "      <th>miaintrp</th>\n",
       "      <th>aialang</th>\n",
       "      <th>wtint2yr</th>\n",
       "      <th>wtmec2yr</th>\n",
       "      <th>sdmvpsu</th>\n",
       "      <th>sdmvstra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31127</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6434.950248</td>\n",
       "      <td>6571.396373</td>\n",
       "      <td>2.0</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31128</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.100000e+01</td>\n",
       "      <td>132.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.77</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9081.700761</td>\n",
       "      <td>8987.041810</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31129</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.500000e+01</td>\n",
       "      <td>189.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.71</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5316.895215</td>\n",
       "      <td>5586.719481</td>\n",
       "      <td>1.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31130</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.500000e+01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29960.839509</td>\n",
       "      <td>34030.994786</td>\n",
       "      <td>2.0</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31131</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.400000e+01</td>\n",
       "      <td>535.0</td>\n",
       "      <td>536.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.65</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26457.708180</td>\n",
       "      <td>26770.584605</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    seqn  sddsrvyr  ridstatr  ridexmon  riagendr      ridageyr  ridagemn  \\\n",
       "0  31127       4.0       2.0       2.0       1.0  5.397605e-79      11.0   \n",
       "1  31128       4.0       2.0       1.0       2.0  1.100000e+01     132.0   \n",
       "2  31129       4.0       2.0       2.0       1.0  1.500000e+01     189.0   \n",
       "3  31130       4.0       2.0       2.0       2.0  8.500000e+01       NaN   \n",
       "4  31131       4.0       2.0       2.0       2.0  4.400000e+01     535.0   \n",
       "\n",
       "   ridageex  ridreth1  dmqmilit  dmdborn  dmdcitzn  dmdyrsus  dmdeduc3  \\\n",
       "0      12.0       3.0       NaN      1.0       1.0       NaN       NaN   \n",
       "1     132.0       4.0       NaN      1.0       1.0       NaN       4.0   \n",
       "2     190.0       4.0       NaN      1.0       1.0       NaN      10.0   \n",
       "3       NaN       3.0       2.0      1.0       1.0       NaN       NaN   \n",
       "4     536.0       4.0       2.0      1.0       1.0       NaN       NaN   \n",
       "\n",
       "   dmdeduc2  dmdschol  dmdmartl  dmdhhsiz  dmdfmsiz  indhhinc  indfminc  \\\n",
       "0       NaN       NaN       NaN       4.0       4.0       4.0       4.0   \n",
       "1       NaN       1.0       NaN       7.0       6.0       8.0       5.0   \n",
       "2       NaN       1.0       5.0       6.0       6.0      10.0      10.0   \n",
       "3       4.0       NaN       2.0       1.0       1.0       4.0       4.0   \n",
       "4       4.0       NaN       1.0       4.0       4.0      11.0      11.0   \n",
       "\n",
       "   indfmpir  ridexprg  dmdhrgnd  dmdhrage  dmdhrbrn  dmdhredu  dmdhrmar  \\\n",
       "0      0.75       NaN       2.0      21.0       1.0       3.0       1.0   \n",
       "1      0.77       2.0       1.0      47.0       1.0       2.0       NaN   \n",
       "2      2.71       NaN       1.0      41.0       1.0       4.0       1.0   \n",
       "3      1.99       NaN       2.0      85.0       1.0       4.0       2.0   \n",
       "4      4.65       2.0       1.0      36.0       1.0       5.0       1.0   \n",
       "\n",
       "   dmdhsedu  sialang  siaproxy  siaintrp  fialang  fiaproxy  fiaintrp  \\\n",
       "0       2.0      1.0       1.0       2.0      1.0       2.0       2.0   \n",
       "1       NaN      1.0       1.0       2.0      1.0       2.0       2.0   \n",
       "2       4.0      1.0       1.0       2.0      1.0       2.0       2.0   \n",
       "3       NaN      1.0       2.0       2.0      1.0       2.0       2.0   \n",
       "4       4.0      1.0       2.0       2.0      1.0       2.0       2.0   \n",
       "\n",
       "   mialang  miaproxy  miaintrp  aialang      wtint2yr      wtmec2yr  sdmvpsu  \\\n",
       "0      NaN       NaN       NaN      NaN   6434.950248   6571.396373      2.0   \n",
       "1      1.0       2.0       2.0      1.0   9081.700761   8987.041810      1.0   \n",
       "2      1.0       2.0       2.0      1.0   5316.895215   5586.719481      1.0   \n",
       "3      NaN       NaN       NaN      NaN  29960.839509  34030.994786      2.0   \n",
       "4      1.0       2.0       2.0      1.0  26457.708180  26770.584605      1.0   \n",
       "\n",
       "   sdmvstra  \n",
       "0      44.0  \n",
       "1      52.0  \n",
       "2      51.0  \n",
       "3      46.0  \n",
       "4      48.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall in the last notebook we read this from the NHANES website\n",
    "demo_df = pd.read_hdf(os.path.join(data_dir, hdf_path), 'demographics_with_sample_weights')\n",
    "\n",
    "# Peek inside\n",
    "demo_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What in the world? What are all the cryptic column names?? Well, the CDC has chosen somewhat unhelpful names that map to demographic characteristics of participants. We *could* go to the website that lists the variable names and keep track of them. However, if we are clever there might be a better way. . . .The pandas library makes it easy to scrape tables via a url. All we need is the url where the table is located, as well as the index of the xml blob we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable Name</th>\n",
       "      <th>Variable Description</th>\n",
       "      <th>Data File Name</th>\n",
       "      <th>Data File Description</th>\n",
       "      <th>Begin Year</th>\n",
       "      <th>EndYear</th>\n",
       "      <th>Component</th>\n",
       "      <th>Use Constraints</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AIALANG</td>\n",
       "      <td>Language of the MEC ACASI Interview Instrument</td>\n",
       "      <td>DEMO_D</td>\n",
       "      <td>Demographic Variables &amp; Sample Weights</td>\n",
       "      <td>2005</td>\n",
       "      <td>2006</td>\n",
       "      <td>Demographics</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DMDBORN</td>\n",
       "      <td>In what country {were you/was SP} born?</td>\n",
       "      <td>DEMO_D</td>\n",
       "      <td>Demographic Variables &amp; Sample Weights</td>\n",
       "      <td>2005</td>\n",
       "      <td>2006</td>\n",
       "      <td>Demographics</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DMDCITZN</td>\n",
       "      <td>{Are you/Is SP} a citizen of the United States...</td>\n",
       "      <td>DEMO_D</td>\n",
       "      <td>Demographic Variables &amp; Sample Weights</td>\n",
       "      <td>2005</td>\n",
       "      <td>2006</td>\n",
       "      <td>Demographics</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DMDEDUC2</td>\n",
       "      <td>(SP Interview Version) What is the highest gra...</td>\n",
       "      <td>DEMO_D</td>\n",
       "      <td>Demographic Variables &amp; Sample Weights</td>\n",
       "      <td>2005</td>\n",
       "      <td>2006</td>\n",
       "      <td>Demographics</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DMDEDUC3</td>\n",
       "      <td>(SP Interview Version) What is the highest gra...</td>\n",
       "      <td>DEMO_D</td>\n",
       "      <td>Demographic Variables &amp; Sample Weights</td>\n",
       "      <td>2005</td>\n",
       "      <td>2006</td>\n",
       "      <td>Demographics</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Variable Name                               Variable Description  \\\n",
       "0       AIALANG     Language of the MEC ACASI Interview Instrument   \n",
       "1       DMDBORN            In what country {were you/was SP} born?   \n",
       "2      DMDCITZN  {Are you/Is SP} a citizen of the United States...   \n",
       "3      DMDEDUC2  (SP Interview Version) What is the highest gra...   \n",
       "4      DMDEDUC3  (SP Interview Version) What is the highest gra...   \n",
       "\n",
       "  Data File Name                   Data File Description  Begin Year  EndYear  \\\n",
       "0         DEMO_D  Demographic Variables & Sample Weights        2005     2006   \n",
       "1         DEMO_D  Demographic Variables & Sample Weights        2005     2006   \n",
       "2         DEMO_D  Demographic Variables & Sample Weights        2005     2006   \n",
       "3         DEMO_D  Demographic Variables & Sample Weights        2005     2006   \n",
       "4         DEMO_D  Demographic Variables & Sample Weights        2005     2006   \n",
       "\n",
       "      Component Use Constraints  \n",
       "0  Demographics            None  \n",
       "1  Demographics            None  \n",
       "2  Demographics            None  \n",
       "3  Demographics            None  \n",
       "4  Demographics            None  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_metadata_url = 'https://wwwn.cdc.gov/nchs/nhanes/search/variablelist.aspx?Component=Demographics&CycleBeginYear=2005'\n",
    "idx = 1 # I looked up the blob I was interested in in advance\n",
    "\n",
    "# Create demographic metadata DataFrame\n",
    "demo_metadata = pd.read_html(demo_metadata_url)[idx]\n",
    "\n",
    "# Filter metadata to just include the demographics table\n",
    "demo_metadata = demo_metadata[demo_metadata['Data File Name']=='DEMO_D']\n",
    "\n",
    "# Peek inside\n",
    "demo_metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Now we have a dataframe with two columns of interest: ```Variable Name``` and ```Variable Description```. We could remap the column names to human readable. I'll leave that as an exercise for you if you feel inclined. The metadata table tells us what the varible name codes refer to but unfortunately they don't tell us what the specific factor levels of each variable map to. In order to get that information we need to refer to the [NHANES documentation](https://wwwn.cdc.gov/nchs/nhanes/2005-2006/DEMO_D.htm#RIDEXMON).\n",
    "\n",
    "Now that we have the demographic data let's join it to the ```pax``` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The demo seqn value is an int and the pax seqn is a string represented float\n",
    "# thus the join will fail if we don't convert.\n",
    "# TODO fix seqn in pax_raw before writing clean to fix this situation\n",
    "demo_df['seqn'] = demo_df.seqn.astype(float).astype(str)\n",
    "\n",
    "# Merge pax and demo into a single DataFrame\n",
    "analysis_set = pax.merge(demo_df, on='seqn', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating a new DataFrame with pax + demographics\n",
    "analysis_set = pax.merge(demo_df, on='seqn', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
